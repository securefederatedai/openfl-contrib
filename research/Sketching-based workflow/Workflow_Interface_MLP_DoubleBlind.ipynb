{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14821d97",
   "metadata": {},
   "source": [
    "# OpenFL Workflow Interface Sketching-based Federated Learning\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/securefederatedai/openfl-contrib/blob/main/research/template/Workflow_Interface_Template.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c3d86a",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The goal of this federated‐learning experiment is to evaluate whether CountSketch‐based compression can dramatically cut down on communication and compute costs in a simple MNIST classification task, while still preserving nearly the same accuracy. Specifically, we want to:\n",
    "\n",
    "1. **Integrate Sketching into OpenFL**  \n",
    "   - Leverage OpenFL’s Workflow API to orchestrate a secure, multi-client training loop.  \n",
    "   - Ensure that each collaborator only exchanges compressed (“sketched”) model updates.\n",
    "2. **Measure Communication Savings**  \n",
    "   - Compare data sent per round using sketched updates versus full updates.  \n",
    "   - Evaluate multiple compression ratios to quantify bandwidth reduction.\n",
    "3. **Validate Accuracy Retention**  \n",
    "   - Ensure that even at aggressive compression levels (e.g. large $q$ ), our MLP on MNIST maintains reasonable test accuracy compared to the uncompressed baseline.\n",
    "4. **Assess Computational Overhead**  \n",
    "   - Track end-to-end training and inference times to demonstrate runtime improvements on both clients and server.\n",
    "5. **Demonstrate Privacy Benefits**  \n",
    "   - Highlight how random hashing and sign-flipping in CountSketch inherently obfuscate model updates, providing a basic privacy layer.\n",
    "\n",
    "By achieving these objectives, we want to show that sketching is a practical, low-overhead method for bandwidth-efficient, privacy-aware federated learning.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8e35da",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Install necessary dependencies and import required libraries. For maintainability, it is recommend to pin your version of OpenFL and the corresponding work_interface_requirements.txt file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f98600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install openfl==1.8.0\n",
    "%pip install -r https://raw.githubusercontent.com/securefederatedai/openfl/refs/tags/v1.8/openfl-tutorials/experimental/workflow/workflow_interface_requirements.txt\n",
    "%pip install -r requirements.txt\n",
    "%pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7237eac4",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "Define your data loaders, model, optimizer, and any helper functions needed for your experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58311e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Check if GPU (CUDA) is available, else use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ========== Data Loading ==========\n",
    "mnist_train = torchvision.datasets.MNIST(\n",
    "    \"./files/\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ]),\n",
    ")\n",
    "\n",
    "mnist_test = torchvision.datasets.MNIST(\n",
    "    \"./files/\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ]),\n",
    ")\n",
    "\n",
    "# ========== Configuration ==========\n",
    "batch_size_train = 16\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c9f2c7",
   "metadata": {},
   "source": [
    "# `Sketch()` Opertations\n",
    "`rand_hashing(n, q)`: to generate hash indices and signs for implicitly computing the sketching matrix $\\mathbf{S}\\in\\mathbb{R}^{n\\times s}$, where $s=\\mathsf{int}(nq)$. \n",
    "\n",
    "`count_sketch(A,S)`: to compute the sketched matrix $\\mathbf{A}^\\prime=\\mathbf{AS}\\in\\mathbb{R}^{m\\times s}$ from the high dimensional matrix $\\mathbf{A}\\in\\mathbb{R}^{m\\times n}$ in $\\mathsf{nnz}(\\mathbf{X})$ time.\n",
    "\n",
    "`transpose_countsketch(A',S)`: to appximately recover the original matrix $\\mathbf{A}\\approx\\mathbf{A}^\\prime\\,\\mathbf{S}^\\top$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea2ba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sketch():\n",
    "   \n",
    "    @staticmethod\n",
    "    def rand_hashing(n, q, seed=None):\n",
    "        \"\"\"\n",
    "        Generate random hashing indices and random sign vector for sketching.\n",
    "\n",
    "        Supports fractional compression ratio q >= 1.0 by defining an effective integer q_eff.\n",
    "\n",
    "        Args:\n",
    "            n (int): Number of items to be hashed (length of the input vector).\n",
    "            q (float >= 1.0): Compression ratio; q = 1.0 means no compression.\n",
    "\n",
    "        Returns:\n",
    "            hash_idx (torch.LongTensor): A tensor of shape (q_eff, s) containing indices for sketching,\n",
    "                where s = floor(n / q) (clamped to [1, n]) and q_eff = floor(q) if q ≥ 2.0 else 1.\n",
    "            rand_sgn (torch.FloatTensor): A tensor of shape (n,) of random ±1 signs for each item.\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: If q < 1.0.\n",
    "        \"\"\"\n",
    "        assert q >= 1.0, \"q must be >= 1.0\"\n",
    "        s = int(n / q)  # Target sketch size (truncated to integer)\n",
    "        s = max(1, min(s, n))  # Clamp to valid range [1, n]\n",
    "        q_eff = math.floor(q) if q >= 2.0 else 1  # Effective q for reshaping\n",
    "\n",
    "        # Save original RNG state and set seed if provided\n",
    "        original_rng_state = torch.random.get_rng_state()\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "\n",
    "        t = torch.randperm(n)\n",
    "        # Handle cases where s*q_eff might exceed n\n",
    "        max_possible = min(s * q_eff, n)\n",
    "        hash_idx = t[:max_possible].reshape((q_eff, -1))  # Flexible reshaping\n",
    "        rand_sgn = torch.randint(0, 2, (n,)).float() * 2 - 1\n",
    "\n",
    "        # Restore original RNG state\n",
    "        if seed is not None:\n",
    "            torch.random.set_rng_state(original_rng_state)\n",
    "       \n",
    "        return hash_idx.to(device), rand_sgn.to(device)\n",
    "   \n",
    "    @staticmethod\n",
    "    def countsketch(a, hash_idx, rand_sgn):\n",
    "        \"\"\"\n",
    "        Apply the CountSketch transform to a matrix.\n",
    "\n",
    "        Args:\n",
    "            a (torch.Tensor): Input matrix of shape (m, n), where each row is an item to sketch.\n",
    "            hash_idx (torch.LongTensor): Index tensor from rand_hashing, shape (q_eff, s).\n",
    "            rand_sgn (torch.FloatTensor): Random sign tensor, shape (n,).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Sketch matrix of shape (m, s) obtained by summing hashed entries with signs.\n",
    "        \"\"\"\n",
    "        m, n = a.shape\n",
    "        s = hash_idx.shape[1]\n",
    "        b = a.mul(rand_sgn)\n",
    "        c = torch.sum(b[:, hash_idx], dim=1)\n",
    "        return c\n",
    "   \n",
    "    @staticmethod\n",
    "    def transpose_countsketch(c, hash_idx, rand_sgn):\n",
    "        \"\"\"\n",
    "            Approximate inverse of CountSketch to reconstruct the original matrix shape.\n",
    "\n",
    "        Args:\n",
    "            c (torch.Tensor): Sketch matrix of shape (m, s).\n",
    "            hash_idx (torch.LongTensor): Index tensor from rand_hashing, shape (q_eff, s).\n",
    "            rand_sgn (torch.FloatTensor): Random sign tensor, shape (n,).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Reconstructed matrix of shape (m, n) approximating the original data.\n",
    "\n",
    "        Notes:\n",
    "            - For q_eff > 1, repeats sketch columns across q blocks to fill n.\n",
    "            - Applies the random signs to recover approximate original values.\n",
    "        \"\"\"\n",
    "        m, s = c.shape\n",
    "        n = len(rand_sgn)\n",
    "        q_eff = hash_idx.shape[0]  # Get q from hash_idx shape\n",
    "       \n",
    "        b = torch.zeros([m, n], dtype=torch.float32).to(device)\n",
    "        if q_eff > 1:\n",
    "            q = n // s  # Infer original q\n",
    "            idx = torch.repeat_interleave(torch.arange(s), q, dim=-1)\n",
    "            selected = hash_idx.T.reshape((-1,))\n",
    "            b[:, selected] = c[:, idx]\n",
    "        else:\n",
    "            b[:, hash_idx[0]] = c  # Special case when q_eff=1\n",
    "           \n",
    "        b = b.mul(rand_sgn)\n",
    "        return b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3895231e",
   "metadata": {},
   "source": [
    "# Custom `forward` and `backward` passes on the sketched space\n",
    "`forward`: Computes the sketched output $\\mathbf{Y}= \\mathbf{X}\\mathbf{S}\\mathbf{S}^\\top\\mathbf{W}+\\mathsf{bias}$, where $\\mathbf{X}$ is input activation and $\\mathbf{W}$ is the weight matrix.\n",
    "\n",
    "`backward`: Computes the gradients--\n",
    "\n",
    "w.r.t. the sketched weights as $\\frac{\\delta\\mathcal{L}}{\\delta \\mathbf{W}^\\prime}=(\\frac{\\delta\\mathcal{L}}{\\delta \\mathbf{Y}})^\\top\\mathbf{X}^\\prime$ (for updating the parameters),\n",
    "\n",
    "w.r.t. the input activation as $\\frac{\\delta\\mathcal{L}}{\\delta \\mathbf{X}}=(\\frac{\\delta\\mathcal{L}}{\\delta \\mathbf{Y}}\\cdot\\mathbf{W}^\\prime)\\cdot\\mathbf{S}^\\top$  (for backpropagation),\n",
    "\n",
    "where $\\mathbf{W}^\\prime=\\mathbf{WS}$ and $\\mathbf{X}^\\prime=\\mathbf{XS}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2608daf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SketchLinearFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "        Forward pass for the Sketch-based linear layer.\n",
    "\n",
    "        When in training mode, applies CountSketch compression to both input and weight,\n",
    "        performs a compressed matrix multiplication, and adds bias. In evaluation mode,\n",
    "        runs a standard linear transform.\n",
    "\n",
    "        Args:\n",
    "            ctx: Context object to save information for backward computation.\n",
    "            input (Tensor): Input tensor of shape (batch_size, input_features).\n",
    "            weight (Tensor): Weight parameter of shape (output_features, input_features).\n",
    "            bias (Tensor): Bias parameter of shape (output_features,).\n",
    "            hash_idx (LongTensor): Precomputed indices for CountSketch, shape (q_eff, s).\n",
    "            rand_sgn (FloatTensor): Precomputed random signs, shape (input_features,).\n",
    "            training (bool): If True, uses sketch-based forward for training; if False, uses exact (for inference).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, output_features).\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, sketched_weight, bias, hash_idx, rand_sgn, training=True):\n",
    "        if training:\n",
    "            # Sketch input and compute output directly in sketched space\n",
    "            input_sketch = Sketch.countsketch(input, hash_idx, rand_sgn)\n",
    "            output = input_sketch @ sketched_weight.t() + bias\n",
    "            \n",
    "            # Store only what's needed for backward (all in sketched space)\n",
    "            ctx.save_for_backward(input_sketch, sketched_weight, bias)\n",
    "            ctx.hash_idx = hash_idx\n",
    "            ctx.rand_sgn = rand_sgn\n",
    "        else:\n",
    "            # For inference (not used in FL)\n",
    "            output = input @ Sketch.transpose_countsketch(sketched_weight, hash_idx, rand_sgn).t() + bias\n",
    "        \n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        Compute gradients while maintaining sketching\n",
    "        Returns:\n",
    "            grad_input: sketched input gradient\n",
    "            grad_weight: sketched weight gradient  \n",
    "            grad_bias: normal bias gradient\n",
    "        \"\"\"\n",
    "        input_sketch, sketched_weight, bias = ctx.saved_tensors\n",
    "        hash_idx = ctx.hash_idx\n",
    "        rand_sgn = ctx.rand_sgn\n",
    "        \n",
    "        # Compute gradients in sketched space\n",
    "        grad_sketched_weight = grad_output.t() @ input_sketch\n",
    "        grad_bias = grad_output.sum(0)\n",
    "        \n",
    "        # Sketch the input gradient\n",
    "        grad_input_sketch = grad_output @ sketched_weight\n",
    "        grad_input = Sketch.transpose_countsketch(grad_input_sketch, hash_idx, rand_sgn)\n",
    "        \n",
    "        return grad_input, grad_sketched_weight, grad_bias, None, None, None\n",
    "\n",
    "\n",
    "class SketchLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    A linear layer that applies CountSketch compression during training.\n",
    "    Layer that ONLY stores and operates on sketched weights\n",
    "    Never contains full weights at any point\n",
    "        Args:\n",
    "            input_features (int): Size of each input sample.\n",
    "            output_features (int): Size of each output sample.\n",
    "            q (float): Compression ratio for sketching (>=1.0).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_features, output_features, q=2):\n",
    "        super(SketchLinear, self).__init__()\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "        self.q = q\n",
    "        \n",
    "        # Initialize bias normally (small, so no sketching needed)\n",
    "        self.bias = nn.Parameter(torch.Tensor(output_features))\n",
    "        \n",
    "        # Will be initialized by federated flow\n",
    "        self.sketched_weight = None  \n",
    "        self.hash_idx = None\n",
    "        self.rand_sgn = None\n",
    "        \n",
    "        # Initialize bias\n",
    "        bound = 1 / math.sqrt(input_features)\n",
    "        self.bias.data.uniform_(-bound, bound)\n",
    "\n",
    "    def init_sketched_weights(self, hash_idx, rand_sgn):\n",
    "        \"\"\"Initialize with properly sized random sketched weights\"\"\"\n",
    "        self.hash_idx = hash_idx\n",
    "        self.rand_sgn = rand_sgn\n",
    "        sketch_dim = hash_idx.shape[1]  # Get target sketch size\n",
    "        self.sketched_weight = nn.Parameter(\n",
    "            torch.randn(self.output_features, sketch_dim) * 0.01\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        # During training: always use sketched operations\n",
    "        if self.training:\n",
    "            return SketchLinearFunction.apply(\n",
    "                input, self.sketched_weight, self.bias, \n",
    "                self.hash_idx, self.rand_sgn, self.training\n",
    "            )\n",
    "        # During inference: approximate full weights\n",
    "        else:\n",
    "            approx_weight = Sketch.transpose_countsketch(\n",
    "                self.sketched_weight, self.hash_idx, self.rand_sgn\n",
    "            )\n",
    "            return F.linear(input, approx_weight, self.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f95b10",
   "metadata": {},
   "source": [
    "# Custom MLP Class `MLP_SketchLinear`\n",
    "\n",
    "It replaces the vanilla `nn.Linear` with `SketchLinear` defined above and\n",
    "\n",
    "uses the custom `forward` and `backward` passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39288b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer perceptron with sketch\n",
    "# Args:\n",
    "    #    dim_in: input dimension\n",
    "    #    dim_out: output dimension\n",
    "    #    q: parameter for random hashing in Sketch\n",
    "# Return:\n",
    "    #    log probabilities of the classes\n",
    "class MLP_SketchLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Modified to properly handle sketched initialization\n",
    "    and maintain double-blind properties\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_in, dim_out, q):\n",
    "        super().__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        self.q = q\n",
    "        self.hidden = [1000, 1000]  # Hidden layer sizes\n",
    "\n",
    "        # Initialize layers (but don't init weights yet)\n",
    "        self.input_layer = SketchLinear(dim_in, self.hidden[0], q)\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            SketchLinear(self.hidden[i], self.hidden[i+1], q) \n",
    "            for i in range(len(self.hidden)-1)\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(self.hidden[-1], dim_out)  # Final layer doesn't need sketching\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def init_sketched_weights(self, hash_idxs, rand_sgns):\n",
    "        \"\"\"Initialize all layers with sketched weights\"\"\"\n",
    "        self.input_layer.init_sketched_weights(hash_idxs[0], rand_sgns[0])\n",
    "        for i, layer in enumerate(self.hidden_layers, start=1):\n",
    "            layer.init_sketched_weights(hash_idxs[i], rand_sgns[i])\n",
    "        # Output layer uses normal initialization\n",
    "        nn.init.xavier_uniform_(self.output_layer.weight)\n",
    "        nn.init.zeros_(self.output_layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.dim_in)\n",
    "        x = self.activation(self.input_layer(x))\n",
    "        for layer in self.hidden_layers:\n",
    "            x = self.activation(layer(x))\n",
    "        return self.log_softmax(self.output_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17732437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(network, test_loader):\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = network(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return float(correct) / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd268911",
   "metadata": {},
   "source": [
    "# Aggregation Function\n",
    "Applies vanilla `FedAvg` on the sketched model updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-studio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OpenFL components\n",
    "from copy import deepcopy\n",
    "\n",
    "from openfl.experimental.workflow.interface import FLSpec, Aggregator, Collaborator\n",
    "from openfl.experimental.workflow.runtime import LocalRuntime\n",
    "from openfl.experimental.workflow.placement import aggregator, collaborator\n",
    "\n",
    "def FedAvg(models):\n",
    "    \"\"\"Generic FedAvg that averages all parameters in the state_dict\"\"\"\n",
    "    new_model = deepcopy(models[0])\n",
    "    state_dicts = [model.state_dict() for model in models]\n",
    "    avg_state_dict = {}\n",
    "    for key in state_dicts[0]:\n",
    "        avg_state_dict[key] = torch.mean(torch.stack([sd[key] for sd in state_dicts]), dim=0)\n",
    "    new_model.load_state_dict(avg_state_dict)\n",
    "    return new_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e406db6",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Flow Definition\n",
    "Define the flow of tasks in the federated learning experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-madrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FederatedFlow(FLSpec):\n",
    "\n",
    "    def __init__(self, model=None, optimizer=None, rounds=3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "            self.optimizer = optimizer\n",
    "        else:\n",
    "            self.model = MLP_SketchLinear(dim_in=784, dim_out=10, q=5)\n",
    "\n",
    "            self.optimizer = optim.SGD(self.model.parameters(), lr=learning_rate,\n",
    "                                       momentum=momentum)\n",
    "        self.rounds = rounds\n",
    "        self.current_round = 0 # Initialize here to avoid AttributeError\n",
    "\n",
    "        self.hash_idxs = []\n",
    "        self.rand_sgns = []\n",
    "\n",
    "\n",
    "    def generate_hash_parameters(self):\n",
    "        \"\"\"Generate hashes for all sketch layers\"\"\"\n",
    "        self.hash_idxs = []\n",
    "        self.rand_sgns = []\n",
    "        \n",
    "        # Generate for input layer \n",
    "        h, r = Sketch.rand_hashing(784, self.model.q, seed=753+self.current_round)\n",
    "        self.hash_idxs.append(h)\n",
    "        self.rand_sgns.append(r)\n",
    "        \n",
    "        # Generate for hidden layers\n",
    "        for _ in range(len(self.model.hidden_layers)):\n",
    "            h, r = Sketch.rand_hashing(1000, self.model.q, seed=43+self.current_round)\n",
    "            self.hash_idxs.append(h)\n",
    "            self.rand_sgns.append(r)\n",
    "\n",
    "    \n",
    "    \n",
    "    @aggregator\n",
    "    def start(self):\n",
    "        print(f'Initializing hash parameters and model..')\n",
    "        self.generate_hash_parameters()  # Generate first set of hash parameters\n",
    "        self.collaborators = self.runtime.collaborators\n",
    "        self.private = 10\n",
    "        # self.current_round = 0\n",
    "\n",
    "        # Initialize model with sketches\n",
    "        self.model.init_sketched_weights(self.hash_idxs, self.rand_sgns)\n",
    "\n",
    "        self.next(self.aggregated_model_validation,\n",
    "                  foreach='collaborators',\n",
    "                  hash_idxs=self.hash_idxs,  # Broadcast hash parameters for next round\n",
    "                  rand_sgns=self.rand_sgns,  # Broadcast hash parameters for next round\n",
    "                  exclude=['private'])\n",
    "\n",
    "    \n",
    "    @collaborator\n",
    "    def aggregated_model_validation(self):\n",
    "        print(f'Performing aggregated model validation for collaborator {self.input}')\n",
    "\n",
    "        # Set hash parameters from the received state\n",
    "        self.model.hash_idxs = self.hash_idxs\n",
    "        self.model.rand_sgns = self.rand_sgns\n",
    "\n",
    "        self.agg_validation_score = inference(self.model, self.test_loader)\n",
    "        print(f'{self.input} value of {self.agg_validation_score}')\n",
    "        self.next(self.train)\n",
    "    \n",
    "    @collaborator\n",
    "    def train(self):\n",
    "        # print collaborator name and which round we’re in\n",
    "        print(f'Collaborator {self.input} rounds [{self.current_round+1}/{self.rounds}]')\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        self.model.train()\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=learning_rate,\n",
    "                                   momentum=momentum)\n",
    "        train_losses = []\n",
    "        for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Train Epoch: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    batch_idx * len(data), len(self.train_loader.dataset),\n",
    "                    100. * batch_idx / len(self.train_loader), loss.item()))\n",
    "                self.loss = loss.item()\n",
    "                torch.save(self.model.state_dict(), 'model.pth')\n",
    "                torch.save(self.optimizer.state_dict(), 'optimizer.pth')\n",
    "\n",
    "        epoch_time = time.time() - epoch_start\n",
    "\n",
    "        print(f'Collaborator {self.input} finished local training for Round '\n",
    "              f'{self.current_round+1}/{self.rounds} in {epoch_time:.2f}s')\n",
    "        \n",
    "        self.training_completed = True\n",
    "        self.next(self.local_model_validation)\n",
    "\n",
    "    @collaborator\n",
    "    def local_model_validation(self):\n",
    "        self.local_validation_score = inference(self.model, self.test_loader)\n",
    "        print(\n",
    "            f'Doing local model validation for collaborator {self.input}: {self.local_validation_score}')\n",
    "        self.next(self.join, exclude=['training_completed'])\n",
    "    \n",
    "\n",
    "    @aggregator\n",
    "    def join(self, inputs):\n",
    "        self.average_loss = sum(input.loss for input in inputs) / len(inputs)\n",
    "        self.aggregated_model_accuracy = sum(\n",
    "            input.agg_validation_score for input in inputs) / len(inputs)\n",
    "        self.local_model_accuracy = sum(\n",
    "            input.local_validation_score for input in inputs) / len(inputs)\n",
    "        print(f'Average aggregated model validation values = {self.aggregated_model_accuracy}')\n",
    "        print(f'Average training loss = {self.average_loss}')\n",
    "        print(f'Average local model validation values = {self.local_model_accuracy}')\n",
    "        \n",
    "        # Use the generic FedAvg\n",
    "        self.model = FedAvg([input.model for input in inputs])\n",
    "        \n",
    "        self.optimizer = [input.optimizer for input in inputs][0]\n",
    "        self.current_round += 1\n",
    "        if self.current_round < self.rounds:\n",
    "            # Generate new hash parameters for the next round\n",
    "            self.generate_hash_parameters()\n",
    "            \n",
    "            self.next(self.aggregated_model_validation,\n",
    "                    foreach='collaborators',\n",
    "                    hash_idxs=self.hash_idxs,  # Broadcast new hash parameters\n",
    "                    rand_sgns=self.rand_sgns,  # Broadcast new hash parameters\n",
    "                    exclude=['private'])\n",
    "        else:\n",
    "            self.next(self.end)\n",
    "\n",
    "    @aggregator\n",
    "    def end(self, *args, **kwargs):\n",
    "        print(f'This is the end of the flow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aabf61e",
   "metadata": {},
   "source": [
    "# Setup Participants\n",
    "Define the aggregator and collaborators, and assign private attributes.\n",
    "\n",
    "Private attributes of a particular participant are accessible ONLY to the particular participants through its task and do not get initialized with the FederatedFlow above (e.g. `train_dataloader` and `test_dataloader`). Additionally these private attributes are always filtered out of the current state when transferring from collaborator to aggregator, and vice versa.\n",
    "\n",
    "Users can directly specify a collaborator's private attributes via `collaborator.private_attributes` which is a dictionary where the key is name of the attribute and the value is the object that is made accessible to the collaborator. In the cell below, we define 1 aggregators and 3 collaborators. For each collaborator, we assign a private data loader for their respective train and test datasets.\n",
    "\n",
    "Note that the private attributes are flexible, and a user can choose to pass in a completely different type of object to any of the collaborators or aggregator (with an arbitrary name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Setup participants\n",
    "aggregator = Aggregator()\n",
    "aggregator.private_attributes = {}\n",
    "\n",
    "# Setup collaborators with private attributes\n",
    "collaborator_names = ['Portland', 'Seattle', 'Chandler','Bangalore']\n",
    "collaborators = [Collaborator(name=name) for name in collaborator_names]\n",
    "for idx, collaborator in enumerate(collaborators):\n",
    "    local_train = deepcopy(mnist_train)\n",
    "    local_test = deepcopy(mnist_test)\n",
    "    local_train.data = mnist_train.data[idx::len(collaborators)]\n",
    "    local_train.targets = mnist_train.targets[idx::len(collaborators)]\n",
    "    local_test.data = mnist_test.data[idx::len(collaborators)]\n",
    "    local_test.targets = mnist_test.targets[idx::len(collaborators)]\n",
    "    collaborator.private_attributes = {\n",
    "            'train_loader': torch.utils.data.DataLoader(local_train,batch_size=batch_size_train, shuffle=True),\n",
    "            'test_loader': torch.utils.data.DataLoader(local_test,batch_size=batch_size_train, shuffle=True)\n",
    "    }\n",
    "\n",
    "local_runtime = LocalRuntime(aggregator=aggregator, collaborators=collaborators, backend='single_process')\n",
    "print(f'Local runtime collaborators = {local_runtime.collaborators}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278ad46b",
   "metadata": {},
   "source": [
    "# Run Experiment\n",
    "Execute the federated learning workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a175b4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "best_model = None\n",
    "optimizer = None\n",
    "flflow = FederatedFlow(model, optimizer, rounds=3, checkpoint=False)\n",
    "flflow.runtime = local_runtime\n",
    "flflow.run()\n",
    "\n",
    "# After the flow completes, calculate and print the total time\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"\\nTotal time for federated flow: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7cc8f7",
   "metadata": {},
   "source": [
    "# Results\n",
    "Retrieve and display the results of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863761fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nFinal aggregated model accuracy for {flflow.rounds} rounds of training: {flflow.aggregated_model_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd1558c",
   "metadata": {},
   "source": [
    "# Checkpointing\n",
    "Utilize checkpointing to examine intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443b06e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run another experiment with checkpointing\n",
    "flflow2 = FederatedFlow(model=flflow.model, optimizer=flflow.optimizer, rounds=2, checkpoint=True)\n",
    "flflow2.runtime = local_runtime\n",
    "flflow2.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61a876d",
   "metadata": {},
   "source": [
    "# Analyze Checkpoints\n",
    "Examine the checkpoints to retrieve intermediate data and logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve run ID\n",
    "run_id = flflow2._run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-prime",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Metaflow components\n",
    "from metaflow import Metaflow, Flow, Task, Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-tamil",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available flows\n",
    "m = Metaflow()\n",
    "list(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55ccb19",
   "metadata": {},
   "source": [
    "# Examine Latest Run\n",
    "Look at the latest run and its steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-defendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get latest run\n",
    "f = Flow('FederatedFlow').latest_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display run details\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5efa1ff",
   "metadata": {},
   "source": [
    "# List Steps\n",
    "List the steps executed in the flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-dressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List steps\n",
    "list(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3292b2e0",
   "metadata": {},
   "source": [
    "# Task Details\n",
    "Retrieve details of a specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-latter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get specific task\n",
    "s = Step(f'FederatedFlow/{run_id}/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-posting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display task details\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-double",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List task steps\n",
    "list(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1866b7",
   "metadata": {},
   "source": [
    "# Task Artifacts\n",
    "Examine the data artifacts generated by a task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-maldives",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get task\n",
    "t = Task(f'FederatedFlow/{run_id}/train/9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-hungarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display task\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef877a50",
   "metadata": {},
   "source": [
    "# Task Data\n",
    "Retrieve data artifacts from the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display task data\n",
    "t.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-torture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display specific data\n",
    "t.data.input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9826c45f",
   "metadata": {},
   "source": [
    "# Task Logs\n",
    "Examine the logs generated by the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display stdout\n",
    "print(t.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd962ddc",
   "metadata": {},
   "source": [
    "# Error Logs\n",
    "Examine any error logs generated by the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f439dff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display stderr\n",
    "print(t.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426f2395",
   "metadata": {},
   "source": [
    "# Challenges and Future Directions\n",
    "\n",
    "1. **Backprop Through Sketch**  \n",
    "   - Inverting the sketch on gradients can introduce bias or noise that accumulates.  \n",
    "   - Investigate regularization or correction terms to stabilize training.\n",
    "\n",
    "2. **Memory & Compute Overhead**  \n",
    "   - Sketching and transpose-sketch operations incur extra computation and temporary storage.  \n",
    "   - Profiling and optimizing these steps (via fused kernels or pruning) is critical for real deployments.\n",
    "\n",
    "3. **Privacy & Security Analysis**  \n",
    "   - While sketching obfuscates individual parameters, formal privacy guarantees (e.g. differential privacy) remain to be studied.  \n",
    "   - Evaluate resistance to inversion attacks or information leakage through repeated sketches.\n",
    "\n",
    "---\n",
    "\n",
    "By addressing these next steps and challenges, we can broaden SketchFL from a proof-of-concept on MNIST to a robust, production-ready framework for sketch-based federated learning on real-world convolutional architectures.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sketching-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
